from keras.models import Sequential
from keras.optimizers import RMSprop
from keras.callbacks import EarlyStopping, TensorBoard
from keras.layers import Dense, Dropout, Flatten, Convolution1D
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import pandas as pd


def ngramCNN():
    train = pd.read_csv("./Data/train/train.csv", header=None, sep="|", names=["raw_str"])
    label = pd.read_csv("./Data/train/train_label.csv", sep=",")
    # test = pd.read_csv("./Data/test/test.csv", header=None, sep="|", names=["raw_str"])

    train_sub = train.sample(frac=0.1, replace=True, random_state=5242)
    label_sub = label.sample(frac=0.1, replace=True, random_state=5242)
    label_sub.drop('sample_id', axis=1, inplace=True)
    print('Shape of the sub train data: ', train_sub.shape)
    print('Shape of the sub label data: ', label_sub.shape)
    # print('Shape of the test data: ', test.shape)

    print(train_sub.head())

    # Apply ngram and Tfidf to
    tfidf = TfidfVectorizer(analyzer="word", max_features=5000, ngram_range=(2, 4))

    print(tfidf)
    train_transformed = tfidf.fit_transform(train_sub.raw_str)
    train_transformed = train_transformed.toarray()  # reshape (N, 5000) to (N, 5000, 1)
    train_transformed = train_transformed.reshape(
        (train_transformed.shape[0], train_transformed.shape[1], 1))

    # test_transformed = tfidf.fit_transform(test.raw_str)

    X_train, X_val, y_train, y_val = train_test_split(train_transformed,
                                                      label_sub,
                                                      test_size=0.05,
                                                      random_state=5242)

    X_max_len = X_train.shape[1]
    print('Shape of the sub train data: ', X_train.shape)
    print('Shape of the sub train label: ', y_train.shape)

    tensorBoardCallback = TensorBoard(log_dir='./logs/ngram_cnn', write_graph=True)

    # model = SentimentAnalysisConv(X_train.shape[1])

    model = Sequential()

    # embedding_layer = Embedding(1, 1, input_length=X_train.shape[1], mask_zero=False)
    # model.add(embedding_layer)
    model.add(Convolution1D(128, 50, input_shape=(X_max_len, 1), strides=50, padding='same'))
    model.add(Flatten())
    model.add(Dropout(0.2))
    model.add(Dense(128, activation='sigmoid'))
    model.add(Dropout(0.2))
    model.add(Dense(1, activation='sigmoid'))

    model.summary()

    early_stopping = EarlyStopping(monitor='val_loss', patience=3)

    model.compile(loss='binary_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])

    model.fit(X_train,
              y_train,
              batch_size=64,
              epochs=10,
              verbose=1,
              callbacks=[early_stopping, tensorBoardCallback],
              validation_data=(X_val, y_val))

    score, acc = model.evaluate(X_val, y_val, verbose=2, batch_size=64)
    print("score: %.2f" % (score))
    print("acc: %.2f" % (acc))
