import numpy as np
from keras.models import Model
from keras.layers import *
from keras.engine import Input
from keras.regularizers import *


# from decov import DeCov

def LSTMAttension(maxlen, embed_size, S):

    main_input = Input(shape=(maxlen,), dtype='int32',   name='main_input')

    emb = Embedding(256, embed_size, input_length=maxlen,   dropout=0.2, W_regularizer=l2(1e-4))(main_input)
    hs = [] #hidden states from each LSTM layer stored here
    hs.append(LSTM(S, dropout_W=0.5, dropout_U=0.5, W_regularizer=regularizers.l2(1e-5), U_regularizer=l2(1e-5), return_sequences=True)(emb))
    num_layers = 3
    for l in range(1, num_layers):
        hs.append(LSTM(S, dropout_W=0.5, dropout_U=0.5, W_regularizer=l2(1e-5), U_regularizer=l2(1e-5), return_sequences=True)(hs[-1]))

    local_states = Concatenate(hs)
    average_active = AverageAcrossTime()(local_states) #this produces h
    state_size = S*num_layers

    #Attention mechanism starts here
    attn_cntx = Concatenate([local_states, RepeatVector(maxlen)(average_active)])
    attn_cntx = TimeDistributed(Dense(S, activation='linear', W_regularizer=l2(1e-4)))(attn_cntx)
    attn_cntx = TimeDistributed(BatchNormalization())(attn_cntx)
    attn_cntx = TimeDistributed(Activation('tanh'))(attn_cntx)
    attn_cntx = TimeDistributed(Dropout(0.5))(attn_cntx)
    attn = TimeDistributed(Dense(1, activation='linear', W_regularizer=l2(1e-4)))(attn_cntx)# α i

    attn = Flatten()(attn)
    attn = Activation('softmax')(attn) # αi
    attn = Reshape((maxlen, 1))(attn)
    attn = TileOut(state_size)(attn) #repeats value to make a specific shape
    final_context = merge([attn, local_states], mode='mul')
    final_context = SumAcrossTime()(final_context) # eq(2), Ti=1 αihi
    final_context = Dense(state_size, activation='linear', W_regularizer=l2(1e-4))(final_context)
    final_context = BatchNormalization()(final_context)
    final_context = Activation('tanh')(final_context)
    final_context = Dropout(0.5)(final_context)
    loss_out = Dense(1, activation='sigmoid',name='loss_out')(final_context)
    model = Model(input=[main_input], output=[loss_out])
    # optimizer = Adam(lr=0.001, clipnorm=1.0)
    # model.compile(optimizer, loss='binary_crossentropy')

    return model
